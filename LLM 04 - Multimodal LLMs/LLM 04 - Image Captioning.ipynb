{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {
    "application/vnd.databricks.v1+cell": {
     "cellMetadata": {},
     "inputWidgets": {},
     "nuid": "19e7ec16-a5fa-49d0-8b76-ad37c4b9f768",
     "showTitle": false,
     "title": ""
    }
   },
   "source": [
    "\n",
    "<div style=\"text-align: center; line-height: 0; padding-top: 9px;\">\n",
    "  <img src=\"https://databricks.com/wp-content/uploads/2018/03/db-academy-rgb-1200px.png\" alt=\"Databricks Learning\" style=\"width: 600px\">\n",
    "</div>"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "application/vnd.databricks.v1+cell": {
     "cellMetadata": {},
     "inputWidgets": {},
     "nuid": "840ebe23-41db-4876-85f4-e3476c8afc7f",
     "showTitle": false,
     "title": ""
    }
   },
   "source": [
    "\n",
    "# Image Captioning Model\n",
    "In the realm of multi-modal large language models (MLLMs), image-to-text and text-to-image models are among one of the most commonly seen applications. Can we generate a caption given an image and vice versa? We saw in the presentation that transformer architectures are general sequence-processing architectures that can process both images and text. In this lesson, we will learn how to build a vision-text transformer architecture yourself and compare the DIY-transformer performance against a pre-trained image captioning model. \n",
    "\n",
    "### ![Dolly](https://files.training.databricks.com/images/llm/dolly_small.png) Learning Objectives\n",
    "1. Connect pretrained vision and text models as a single transformer architecture for image captioning\n",
    "1. Train your image captioning model\n",
    "1. Qualitatively inspect the quality of the caption generated\n",
    "1. Compare the output with an pretrained image captioning model"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "application/vnd.databricks.v1+cell": {
     "cellMetadata": {},
     "inputWidgets": {},
     "nuid": "7274dc79-7ef2-4389-a8c7-c7914ffcd014",
     "showTitle": false,
     "title": ""
    }
   },
   "source": [
    "## Classroom Setup"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 0,
   "metadata": {
    "application/vnd.databricks.v1+cell": {
     "cellMetadata": {},
     "inputWidgets": {},
     "nuid": "56c34f8d-ef0d-4a66-af91-301717dbf538",
     "showTitle": false,
     "title": ""
    }
   },
   "outputs": [],
   "source": [
    "%run ../Includes/Classroom-Setup"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "application/vnd.databricks.v1+cell": {
     "cellMetadata": {},
     "inputWidgets": {},
     "nuid": "05d4b504-05a2-44f0-aa1b-92e386dfdd37",
     "showTitle": false,
     "title": ""
    }
   },
   "source": [
    "We are going to use `sbu_captions` as our dataset to train our own image captioning model. \n",
    "\n",
    "`sbu_captions` contains 1 million pairs of image urls and captions. Visit [this link](https://huggingface.co/datasets/sbu_captions) to view their dataset on Hugging Face Datasets hub."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 0,
   "metadata": {
    "application/vnd.databricks.v1+cell": {
     "cellMetadata": {},
     "inputWidgets": {},
     "nuid": "f4a42de4-f229-4973-b8fc-ab54ce3cf884",
     "showTitle": false,
     "title": ""
    }
   },
   "outputs": [],
   "source": [
    "from datasets import load_dataset\n",
    "\n",
    "data = load_dataset(\"sbu_captions\", split=\"train\").shuffle(seed=42)\n",
    "data"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "application/vnd.databricks.v1+cell": {
     "cellMetadata": {},
     "inputWidgets": {},
     "nuid": "54f8d9ff-f1cb-4fd0-baa3-293675f4e4f8",
     "showTitle": false,
     "title": ""
    }
   },
   "source": [
    "## Initialize an image-to-text model\n",
    "\n",
    "In the following section, we are going to use [Vision Encoder Decoder Model](https://huggingface.co/docs/transformers/model_doc/vision-encoder-decoder) to initialize our image-to-text model. The encoder would be a transformer-based vision model to process the images and the decoder is a language model to generate the caption. Then, we will connect both the vision and language model together and further train the vision-language model on a subset of `sbu_captions`. \n",
    "\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "application/vnd.databricks.v1+cell": {
     "cellMetadata": {},
     "inputWidgets": {},
     "nuid": "36ca67b5-8a41-40aa-8890-8ed755822d22",
     "showTitle": false,
     "title": ""
    }
   },
   "source": [
    "### Define data processing function\n",
    "\n",
    "First, we will need to have a function to properly process our training dataset that contains the image urls and respective captions."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 0,
   "metadata": {
    "application/vnd.databricks.v1+cell": {
     "cellMetadata": {},
     "inputWidgets": {},
     "nuid": "31fb8e84-5d7d-4bd4-b431-708fcbdcc511",
     "showTitle": false,
     "title": ""
    }
   },
   "outputs": [],
   "source": [
    "import torch\n",
    "from torch.utils.data import Dataset\n",
    "from PIL import Image\n",
    "import requests\n",
    "from io import BytesIO\n",
    "\n",
    "class ProcessDataset(Dataset):\n",
    "    def __init__(self, df, tokenizer,feature_extractor, decoder_max_length=20):\n",
    "        self.df = df\n",
    "        self.tokenizer = tokenizer # this is for language model \n",
    "        self.feature_extractor = feature_extractor # this is for vision model \n",
    "        self.decoder_max_length = decoder_max_length # this is for caption output\n",
    "\n",
    "    def __len__(self):\n",
    "        # this is necessary so that HuggingFace won't complain that the dataset doesn't have __len__ method \n",
    "        # when it starts training\n",
    "        return len(self.df)\n",
    "\n",
    "    def __getitem__(self, idx):\n",
    "        # this is another method name that HuggingFace expects \n",
    "        # get file name + text \n",
    "        img_path = self.df[\"image_url\"][idx]\n",
    "        caption = self.df[\"caption\"][idx]\n",
    "        \n",
    "        # process image \n",
    "        response = requests.get(img_path)\n",
    "        image = Image.open(BytesIO(response.content))\n",
    "        pixel_values = self.feature_extractor(image, return_tensors=\"pt\").pixel_values\n",
    "\n",
    "        # labels here refer to each token in the caption\n",
    "        labels = self.tokenizer(caption, \n",
    "                                truncation=True,\n",
    "                                padding=\"max_length\", \n",
    "                                max_length=self.decoder_max_length).input_ids\n",
    "\n",
    "        encoding = {\"pixel_values\": pixel_values.squeeze(), \"labels\": torch.tensor(labels)}\n",
    "        return encoding"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "application/vnd.databricks.v1+cell": {
     "cellMetadata": {},
     "inputWidgets": {},
     "nuid": "c9536605-ca4a-41e5-87e6-49e81a3db6a8",
     "showTitle": false,
     "title": ""
    }
   },
   "source": [
    "### Initialize tokenizer and image feature extractor\n",
    "\n",
    "Next, we will initialize our tokenizer to process text and feature extractor to process images respectively. After this, we are ready to pass our training dataset for processing."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 0,
   "metadata": {
    "application/vnd.databricks.v1+cell": {
     "cellMetadata": {},
     "inputWidgets": {},
     "nuid": "4b12a54f-9917-4a1f-94b7-bf6fe5788b92",
     "showTitle": false,
     "title": ""
    }
   },
   "outputs": [],
   "source": [
    "from transformers import GPT2TokenizerFast, ViTFeatureExtractor\n",
    "\n",
    "tokenizer = GPT2TokenizerFast.from_pretrained(\"gpt2\", cache_dir=DA.paths.datasets+\"/models\")\n",
    "# GPT2 doesn't have a pad token \n",
    "tokenizer.pad_token = tokenizer.eos_token\n",
    "\n",
    "feature_extractor = ViTFeatureExtractor.from_pretrained(\"google/vit-base-patch16-224-in21k\", cache_dir=DA.paths.datasets+\"/models\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 0,
   "metadata": {
    "application/vnd.databricks.v1+cell": {
     "cellMetadata": {},
     "inputWidgets": {},
     "nuid": "f3927a37-55b6-4c30-b546-4cd155f0cae9",
     "showTitle": false,
     "title": ""
    }
   },
   "outputs": [],
   "source": [
    "train_dataset = ProcessDataset(df=data[:2000],\n",
    "                               tokenizer=tokenizer,\n",
    "                               feature_extractor=feature_extractor)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "application/vnd.databricks.v1+cell": {
     "cellMetadata": {},
     "inputWidgets": {},
     "nuid": "0a5cc4dc-e2af-465f-8246-4cca7a5ff1d6",
     "showTitle": false,
     "title": ""
    }
   },
   "source": [
    "### Using VisionEncoderDecoder \n",
    "\n",
    "Here, we will finally use `VisionEncoderDecoder` to connect our pretrained image and text models of choice. \n",
    "\n",
    "You might see in the output that some weights of the GPT2 model are not initialized from the model checkpoint; from HuggingFace: `You should probably TRAIN this model on a down-stream task to be able to use it for predictions and inference.` For the best performance, we should ideally fine-tune this decoder on our own dataset separately and load the fine-tuned decoder. However, for simplicity's sake, we are simply going to use the model as is, and fine-tune the image-captioning model as a whole. "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 0,
   "metadata": {
    "application/vnd.databricks.v1+cell": {
     "cellMetadata": {},
     "inputWidgets": {},
     "nuid": "10633f3a-8946-44bf-aa36-9a9e98e8f395",
     "showTitle": false,
     "title": ""
    }
   },
   "outputs": [],
   "source": [
    "from transformers import VisionEncoderDecoderModel\n",
    "\n",
    "model = VisionEncoderDecoderModel.from_encoder_decoder_pretrained\\\n",
    "    (encoder_pretrained_model_name_or_path=\"google/vit-base-patch16-224-in21k\", \n",
    "     decoder_pretrained_model_name_or_path=\"gpt2\", \n",
    "     tie_encoder_decoder=True, cache_dir=DA.paths.datasets+\"/models\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 0,
   "metadata": {
    "application/vnd.databricks.v1+cell": {
     "cellMetadata": {},
     "inputWidgets": {},
     "nuid": "20e98a44-eee9-465a-85d2-cd15732aeb66",
     "showTitle": false,
     "title": ""
    }
   },
   "outputs": [],
   "source": [
    "# GPT2 only has bos/eos tokens but not decoder_start/pad tokens\n",
    "model.config.decoder_start_token_id = tokenizer.bos_token_id\n",
    "model.config.pad_token_id = tokenizer.pad_token_id\n",
    "model.config.eos_token_id = tokenizer.eos_token_id\n",
    "\n",
    "# We will adjust several more model configuration settings here \n",
    "model.config.vocab_size = model.config.decoder.vocab_size\n",
    "model.config.early_stopping = True\n",
    "model.config.no_repeat_ngram_size = 3 # this determines a sequence of N words that cannot be repeated \n",
    "model.config.length_penalty = 2.0\n",
    "\n",
    "# For decoder only \n",
    "model.decoder.num_beams = 4\n",
    "model.decoder.max_length = 20 "
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "application/vnd.databricks.v1+cell": {
     "cellMetadata": {},
     "inputWidgets": {},
     "nuid": "d95dc87f-aa9e-4eb2-99d9-6155f510a4cb",
     "showTitle": false,
     "title": ""
    }
   },
   "source": [
    "## Train image-captioning model\n",
    "\n",
    "Recall that you have seen `data_collator` in Module 2 before. \n",
    "\n",
    "As a refresher, data collators help us form batches of inputs to pass in to the model for training. Go [here](https://huggingface.co/docs/transformers/main/en/main_classes/data_collator#data-collator) for documentation."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 0,
   "metadata": {
    "application/vnd.databricks.v1+cell": {
     "cellMetadata": {},
     "inputWidgets": {},
     "nuid": "80bc3ba9-6674-4007-888e-a87ee8aa18df",
     "showTitle": false,
     "title": ""
    }
   },
   "outputs": [],
   "source": [
    "from transformers import Trainer, TrainingArguments\n",
    "from transformers import default_data_collator\n",
    "import os\n",
    "\n",
    "BATCH_SIZE = 16\n",
    "TRAIN_EPOCHS = 20\n",
    "\n",
    "output_directory = os.path.join(DA.paths.working_dir, \"captioning_outputs\")\n",
    "\n",
    "training_args = TrainingArguments(\n",
    "    output_dir=output_directory,\n",
    "    per_device_train_batch_size=BATCH_SIZE,\n",
    "    do_train=True,\n",
    "    num_train_epochs=TRAIN_EPOCHS, # number of passes to see the entire dataset \n",
    "    overwrite_output_dir=True,\n",
    "    no_cuda=NO_CUDA, # Not using GPU when on CPU-only cluster\n",
    "    dataloader_pin_memory=False # this specifies whether you want to pin memory in data loaders or not\n",
    ")\n",
    "\n",
    "trainer = Trainer(\n",
    "    tokenizer=feature_extractor,\n",
    "    model=model,\n",
    "    args=training_args,\n",
    "    train_dataset=train_dataset,\n",
    "    data_collator=default_data_collator,\n",
    ")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "application/vnd.databricks.v1+cell": {
     "cellMetadata": {},
     "inputWidgets": {},
     "nuid": "f074a7bd-cda9-4c90-a4b8-e83345575728",
     "showTitle": false,
     "title": ""
    }
   },
   "source": [
    "Note: You might notice that this cells triggers a whole new MLflow run. [MLflow](https://mlflow.org/docs/latest/index.html) is an open source tool that helps to manage end-to-end machine learning lifecycle, including experiment tracking, ML code packaging, and model deployment. You can read more about [LLM tracking here](https://mlflow.org/docs/latest/llm-tracking.html)."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 0,
   "metadata": {
    "application/vnd.databricks.v1+cell": {
     "cellMetadata": {},
     "inputWidgets": {},
     "nuid": "b9f2c6d2-627f-4f8f-977e-6e1c7f8d6985",
     "showTitle": false,
     "title": ""
    }
   },
   "outputs": [],
   "source": [
    "import mlflow\n",
    "\n",
    "mlflow.set_experiment(f\"/Users/{DA.username}/LLM 04 - Image Captioning\")\n",
    "\n",
    "trainer.train()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "application/vnd.databricks.v1+cell": {
     "cellMetadata": {},
     "inputWidgets": {},
     "nuid": "1b5eadfd-4e08-420c-b278-33d0cf20f533",
     "showTitle": false,
     "title": ""
    }
   },
   "source": [
    "## Generate caption from an image\n",
    "\n",
    "Now, let's try generating caption from a randomly picked image below."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 0,
   "metadata": {
    "application/vnd.databricks.v1+cell": {
     "cellMetadata": {},
     "inputWidgets": {},
     "nuid": "ceeade69-b3dc-4574-9f20-ce868a24d9fe",
     "showTitle": false,
     "title": ""
    }
   },
   "outputs": [],
   "source": [
    "test_img = data[2021]\n",
    "\n",
    "test_img_path = test_img[\"image_url\"]\n",
    "test_img_response = requests.get(test_img_path)\n",
    "test_image = Image.open(BytesIO(test_img_response.content))\n",
    "display(test_image)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 0,
   "metadata": {
    "application/vnd.databricks.v1+cell": {
     "cellMetadata": {},
     "inputWidgets": {},
     "nuid": "f3cb6cd3-84bb-4a3f-846a-4512e229effb",
     "showTitle": false,
     "title": ""
    }
   },
   "outputs": [],
   "source": [
    "caption = tokenizer.decode(trainer.model.to(\"cpu\").generate(feature_extractor(test_image, return_tensors=\"pt\").pixel_values)[0])\n",
    "print(\"--\"*20)\n",
    "print(caption)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "application/vnd.databricks.v1+cell": {
     "cellMetadata": {},
     "inputWidgets": {},
     "nuid": "f22956a6-21fe-40e1-8a7d-0879910d3277",
     "showTitle": false,
     "title": ""
    }
   },
   "source": [
    "Hmm, you can see that the caption is not really relevant to the image above. What went wrong? There are two possible reasons: \n",
    "- Parts of our text decoder weights were not loaded from the pretrained checkpoints. So the best approach would have been to train the decoder separately on the training dataset first and load the fine-tuned decoder. \n",
    "- Our image-captioning model needs more fine-tuning time! Try increasing the # of epochs, # of training data samples, and adjust other model hyperparameters if you'd like. \n",
    "\n",
    "But now you learned how to plug in your own transformer-based models and connect them as an encoder-decoder model! It would definitely be much easier if we use an existing image captioning model instead. Let's do that next!  "
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "application/vnd.databricks.v1+cell": {
     "cellMetadata": {},
     "inputWidgets": {},
     "nuid": "3ad224a6-5977-4eb3-8d42-19e6df75a523",
     "showTitle": false,
     "title": ""
    }
   },
   "source": [
    "## What if we use an existing image captioning model instead? \n",
    "\n",
    "Will the caption generated improve and be more relevant? Let's find out! \n",
    "\n",
    "We will be using a new model called `BLIP`, which stands for Bootstrapping Language-Image Pre-training. It's a modeling approach to unify vision-language understanding and generation by [Li et al 2022](https://arxiv.org/abs/2201.12086). It stands out from other vision-language model because: \n",
    "\n",
    ">  Most existing pre-trained models only excel in either understanding-based tasks or generation-based tasks. [...] BLIP effectively utilizes the noisy web data by bootstrapping the captions, where a captioner generates synthetic captions and a filter removes the noisy ones.\n",
    "\n",
    "The excerpt above is from the paper's abstract. You can also read briefly about BLIP in the [HuggingFace documentation](https://huggingface.co/Salesforce/blip-image-captioning-base)."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 0,
   "metadata": {
    "application/vnd.databricks.v1+cell": {
     "cellMetadata": {},
     "inputWidgets": {},
     "nuid": "c31184d9-c4d0-4a7e-9714-e371f0873419",
     "showTitle": false,
     "title": ""
    }
   },
   "outputs": [],
   "source": [
    "from transformers import BlipProcessor, BlipForConditionalGeneration\n",
    "\n",
    "blip_processor = BlipProcessor.from_pretrained(\"Salesforce/blip-image-captioning-base\", cache_dir=DA.paths.datasets+\"/models\")\n",
    "blip_model = BlipForConditionalGeneration.from_pretrained(\"Salesforce/blip-image-captioning-base\", cache_dir=DA.paths.datasets+\"/models\")\n",
    "\n",
    "# conditional image captioning\n",
    "# in many of the initial vision-language models, adding a prefix text like below \"a photo of \" is crucial for models to do well\n",
    "# the addition of the prefix text makes the caption generation \"conditional\"\n",
    "text = \"a photo of\"\n",
    "inputs = blip_processor(test_image, text, return_tensors=\"pt\")\n",
    "\n",
    "conditional_output = blip_model.generate(**inputs)\n",
    "print(\"Conditional output: \", blip_processor.decode(conditional_output[0], skip_special_tokens=True))\n",
    "\n",
    "# unconditional image captioning\n",
    "# in newer model iterations, researchers have found improvements to remove the need of adding a prefix text \n",
    "# therefore, the caption generation is \"unconditional\"\n",
    "# notice that the `text` field is no longer filled out (it's now optional)\n",
    "inputs = blip_processor(test_image, return_tensors=\"pt\")\n",
    "\n",
    "unconditional_output = blip_model.generate(**inputs)\n",
    "print(\"Unconditional output: \", blip_processor.decode(unconditional_output[0], skip_special_tokens=True))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "application/vnd.databricks.v1+cell": {
     "cellMetadata": {},
     "inputWidgets": {},
     "nuid": "24ec6b46-5839-4b80-aa59-2f7a1164dcf0",
     "showTitle": false,
     "title": ""
    }
   },
   "source": [
    "Now that you have seen how to generate caption from images, we are going to get our hands dirty to play with videos in the lab! You will learn how to perform zero-shot video classification. "
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "application/vnd.databricks.v1+cell": {
     "cellMetadata": {},
     "inputWidgets": {},
     "nuid": "aa6f790c-b336-4096-a550-19b3d3c61266",
     "showTitle": false,
     "title": ""
    }
   },
   "source": [
    "&copy; 2023 Databricks, Inc. All rights reserved.<br/>\n",
    "Apache, Apache Spark, Spark and the Spark logo are trademarks of the <a href=\"https://www.apache.org/\">Apache Software Foundation</a>.<br/>\n",
    "<br/>\n",
    "<a href=\"https://databricks.com/privacy-policy\">Privacy Policy</a> | <a href=\"https://databricks.com/terms-of-use\">Terms of Use</a> | <a href=\"https://help.databricks.com/\">Support</a>"
   ]
  }
 ],
 "metadata": {
  "application/vnd.databricks.v1+notebook": {
   "dashboards": [],
   "language": "python",
   "notebookMetadata": {},
   "notebookName": "LLM 04 - Image Captioning",
   "widgets": {}
  }
 },
 "nbformat": 4,
 "nbformat_minor": 0
}
